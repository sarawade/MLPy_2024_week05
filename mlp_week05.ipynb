{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce39bd6",
   "metadata": {},
   "source": [
    "# Week 5 - Regularization\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will be able to \n",
    "\n",
    ">* perform regulized regression in sklearn\n",
    ">* understand the role of tuning parameter(s)\n",
    ">* use cross-validation for model tuning and comparison.\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Baseline Model](#baseline)\n",
    "4. [Ridge Regression](#ridge)\n",
    "4. [Lasso Regression](#lasso)\n",
    "4. [ElasticNet Regression](#elasticnet)\n",
    "\n",
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. You should aim to switch roles between driver and navigator approximately every 15 minutes. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21af68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem Definition and Setup<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df530064",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, let's load some of the packages you wil need for this workshop (we will load others as we progress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf44065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fea23b",
   "metadata": {},
   "source": [
    "## User Defined Helper Functions\n",
    "\n",
    "We will make use of the two helper functions that we used last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(m):\n",
    "    \"\"\"Returns the model coefficients from a Scikit-learn model object as an array,\n",
    "    includes the intercept if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If pipeline, use the last step as the model\n",
    "    if (isinstance(m, sklearn.pipeline.Pipeline)):\n",
    "        m = m.steps[-1][1]\n",
    "    \n",
    "    \n",
    "    if m.intercept_ is None:\n",
    "        return m.coef_\n",
    "    \n",
    "    return np.concatenate([[m.intercept_], m.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should fit plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    MSE = mean_squared_error(y, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    Rsqr = r2_score(y, y_hat)\n",
    "    \n",
    "    Metrics = (round(MSE, 4), round(RMSE, 4), round(Rsqr, 4))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Actual vs Fitted plot\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y_hat', y='resid', data=res).set_title(\"Fitted vs Residual plot\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model (MSE, RMSE, Rsqr) = \" + str(Metrics), fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    return Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f3cab",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this week's workshop comes from the Elements of Statistical Learning textbook. The data originally come from a study by [Stamey et al. (1989)](https://www.sciencedirect.com/science/article/abs/pii/S002253471741175X)  in which they examined the relationship between the level of prostate-specific antigen (`psa`) and a number of clinical measures in men who were about to receive a prostatectomy. The variables are as follows,\n",
    "\n",
    "* `lpsa` - log of the level of prostate-specific antigen\n",
    "* `lcavol` - log cancer volume\n",
    "* `lweight` - log prostate weight\n",
    "* `age` - patient age\n",
    "* `lbph` - log of the amount of benign prostatic hyperplasia\n",
    "* `svi` - seminal vesicle invasion\n",
    "* `lcp` - log of capsular penetration\n",
    "* `gleason` - Gleason score\n",
    "* `pgg45` - percent of Gleason scores 4 or 5\n",
    "* `train` - test / train split used in ESL\n",
    "\n",
    "These data are available in `prostate.csv`, which is included in the workshop materials.\n",
    "\n",
    "Let's start by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38009894",
   "metadata": {},
   "outputs": [],
   "source": [
    "prostate = pd.read_csv('prostate.csv')\n",
    "prostate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56091846",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis<a id='eda'></a>\n",
    "\n",
    "Before modelling, we will start with EDA to gain an understanding of the data, through descriptive statistics and visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d47c1a",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "a) Examine the data structure, look at the descriptive statistics, and create a pairs plot. Do any of our variables appear to be categorical / ordinal rather than numeric?\n",
    "\n",
    "b) Are there any interesting patterns in these data? Which variable appears likely to have the strongest relationship with `lpsa`? Why do you think we are exploring the relationship between these variables and `lpsa` (log of psa) rather than just psa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e35d14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8766745e",
   "metadata": {},
   "source": [
    "## Train-Test Set <a id='gen'></a>\n",
    "\n",
    "For these data we have already been provided a column to indicate which values should be used for the training set and which for the test set. This is encoded by the values in the `train` column - we can use these columns to separate our data and generate our training data: `X_train` and `y_train` as well as our test data `X_test` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data frames\n",
    "train = prostate.query(\"train == 'T'\").drop('train', axis=1)\n",
    "test = prostate.query(\"train == 'F'\").drop('train', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = train.drop(['lpsa'], axis=1)\n",
    "y_train = train.lpsa\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "X_test = test.drop('lpsa', axis=1)\n",
    "y_test = test.lpsa\n",
    "\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758bb59",
   "metadata": {},
   "source": [
    "Let's also fix the random seed to make this notebook's output identical at every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610babd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "rng = np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00562f7",
   "metadata": {},
   "source": [
    "# Baseline model<a id='baseline'></a>\n",
    "\n",
    "Our first task is to fit a baseline model which we will be able to use as a point of comparison for our subsequent models. A good candidate for this is a simple linear regression model that includes all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad689d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e9704",
   "metadata": {},
   "source": [
    "We can extract the coefficients for the model, which correspond to the variables: `intercept`, `lcavol`, `lweight`, `age`, `lbph`, `svi`, `lcp`, `gleason`, and `pgg45` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.c_[np.append(['intercept'],lm.feature_names_in_),np.round(get_coefs(lm),4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4bef9",
   "metadata": {},
   "source": [
    "These coefficients have the typical regression interpretation, e.g. for each unit increase in `lcavol` we expect `lpsa` to increase by 0.5765 on average. These values are not of particular interest for us for this particular problem as we are more interested in the predictive properties of our model(s). To evaluate this we will use the `model_fit` helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc64f1a",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 2 (CORE)\n",
    "\n",
    "Use the `model_fit` function to evaluate both the model fit on the training data and the predictions on the test data. \n",
    "\n",
    "- Based on these plots do you see anything in the fit or residual plot that is potentially concerning? \n",
    "- Do you expect the MSE on test data to be better or worse than the MSE on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaceae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2133081",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b6ea42",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "In subsequent sections we will be exploring the use of the Ridge and Lasso regression models which both penalize larger values of $\\mathbf{w}$. While not particularly bad, our baseline model had coefficients that ranged from the smallest at 0.0095 to the largest at 0.737 which is about a 78x difference in magnitude. This difference can be made even worse if we were to change the units of one of our features, e.g. changing a measurement in kg to grams would change that coefficient by 1000 which has no effect on the fit of our linear regression model (predictions and other coefficients would be unchanged) but would have a meaningful impact on the estimates given by a Ridge or Lasso regression model, since that coefficient would now dominate the penalty term.\n",
    "\n",
    "To deal with this issue, the standard approach is to standaridize all features. Additionally, the feature values can now be interpreted as the number of standard deviations each observation is away from that column's mean.\n",
    "Using `sklearn` we can perform this transformation using the `StandardScaler` transformer from the preprocessing submodule.\n",
    "\n",
    "Keep in mind, that in order to get a realistic idea of the performance of model on the test data, **the mean and standard deviation used to standardize both the training and test sets should be computed from the training data only**.  The best way to accomplish this is to include the StandardScaler in a modeling pipeline for your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb755d2",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 3 (CORE)\n",
    "\n",
    "Consider the following pipeline that first standardizes the features before linear regression. Fit the model to the training data.  Using this new model what has changed about our model results? Comment on both the model's coefficients as well as its predictive performance. How has the interpretation of coefficients changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline combine StandardScaler with LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lm_s = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1351de1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c441c845",
   "metadata": {},
   "source": [
    "Note that by adding the `StandardScaler()` step in the pipeline, we have standardized all features, including the binary and ordinal features. This makes interpreting the coefficients of the binary and ordinal features more challenging. Because of this, some practioners prefer to only standardize the numerical variables; in that case, you can use `ColumnTransformer()` to apply standardization only to the numerical variables. \n",
    "\n",
    "We can check the mean and standard deviation used to standardize the features by accessing the `.mean_` and `.scale_` attributes of the `StandardScaler()`. Notice the values used to transform the binary variable `svi`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c54a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X_train)\n",
    "print(np.c_[ss.feature_names_in_,np.round(ss.mean_,4)])\n",
    "print(np.c_[ss.feature_names_in_,np.round(ss.scale_,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f906aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After standardizing, the orginal value of 0 for svi is replaced with',np.round(-ss.mean_[4]/ss.scale_[4],4) )\n",
    "print('After standardizing, the orginal value of 1 for svi is replaced with',np.round((1-ss.mean_[4])/ss.scale_[4],4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d4794",
   "metadata": {},
   "source": [
    "When standardizing all features, if we are interested in interpreting the value of the coefficients, we should **unstandardize** the coefficients. Letting $\\tilde{\\mathbf{x}}$ denote the standardized features and $\\hat{\\mathbf{w}}$ denote the estimated coeffcients when training with standardized features, we have that:\n",
    "\n",
    "$$ \\text{E}[y | \\tilde{\\mathbf{x}}] = \\hat{w}_0 + \\hat{w}_1\\tilde{x}_1 + \\ldots + \\hat{w}_D\\tilde{x}_D.$$\n",
    "\n",
    "Noting that $\\tilde{x}_d = (x_d- \\bar{x}_d)/s_d$ (where $\\bar{x}_d$ and $s_d$ represent the sample mean and standard deviation), we can transform back to the original space:\n",
    "\n",
    "$$ \\text{E}[y | \\mathbf{x}] = \\hat{w}_0 + \\hat{w}_1(x_1-\\bar{x}_1)/s_1 + \\ldots + \\hat{w}_D(x_D-\\bar{x}_D)/s_D.$$\n",
    "\n",
    "Thus, \n",
    "$$ \\text{E}[y | \\mathbf{x}] = \\left( \\hat{w}_0 - \\sum_d \\bar{x}_d/s_d \\right)+ \\hat{w}_1/s_1 x_1+ \\ldots + \\hat{w}_D/s_D  x_D.$$\n",
    "\n",
    "And, the *unstandardized* coefficients are obtain by dividing $\\hat{\\mathbf{w}}$ by the standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d08342",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE)\n",
    "\n",
    "Unstandardize the coefficients and interpret the effect of the binary variable `svi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40e6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "398d473c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b637f6f",
   "metadata": {},
   "source": [
    "# Ridge Regression<a id='ridge'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f3241",
   "metadata": {},
   "source": [
    "Ridge regression is a natural extension to linear regression which introduces an $\\ell_2$ penalty on the coefficients in a standard least squares problem. \n",
    "\n",
    "The [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) model is provided by the `linear_model` submodule. Note that the penalty parameter (referred to as $\\lambda$ in the lecture notes) is called `alpha` is sklearn, and, as discussed in lectures, this parameter crucially determines the amount of shrinkage towards zero and the weight of the $\\ell_2$ penalty.\n",
    "\n",
    "After defining the ridge regression model via, e.g. `Ridge(alpha = 1)`, the usual methods can be called, such as `.fit()` to fit the model and `.predict()` to make predictions. \n",
    "\n",
    "As for the `LinearRegression()`, after fitting, the intercept and coefficients are stored separately in the attributes `.intercept_` and `.coef_`. In Ridge, this is helpful as it highlights how the penalty is only applied to the coefficient (i.e. we do not want to shrink the intercept).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da6af8",
   "metadata": {},
   "source": [
    "Let's start by fitting a ridge regression model with $\\alpha=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ef0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected alpha value \n",
    "alpha_val = 1\n",
    "\n",
    "r = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha = alpha_val)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(np.c_[np.append(['intercept'],r.feature_names_in_),np.round(get_coefs(r),4)])\n",
    "\n",
    "model_fit(r, X_test, y_test, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb454763",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "Adjust the value of `alpha` in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the MSE change? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee863",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe88e20e",
   "metadata": {},
   "source": [
    "\n",
    "## Solution path: Ridge coeffcients as a function of $\\alpha$\n",
    "\n",
    "A useful way of examining the behavior of Ridge regression models is to plot the **solution path** of the coefficents $\\mathbf{w}$ as a function of the penalty parameter $\\alpha$. Since Ridge regression is equivalent to linear regression when $\\alpha=0$, we can see that as we increase the value of $\\alpha$, we are shrinking all of the coefficients in $\\mathbf{w}$ towards zero asymptotically $\\alpha$ approaches infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d042ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 3, num=200) # from 10^-2 to 10^3\n",
    "\n",
    "ws = [] # Store coefficients\n",
    "mses_train = [] # Store training mses\n",
    "mses_test = [] # Store test mses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge(alpha=a)\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    ws.append(m[1].coef_) \n",
    "    mses_train.append(mean_squared_error(y_train, m.predict(X_train)))\n",
    "    mses_test.append(mean_squared_error(y_test, m.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame for plotting\n",
    "sol_path = pd.DataFrame(\n",
    "    data = ws,\n",
    "    columns = X_train.columns # Label columns w/ feature names\n",
    ").assign(\n",
    "    alpha = alphas,\n",
    ").melt(\n",
    "    id_vars = ('alpha')\n",
    ")\n",
    "\n",
    "ax = sns.lineplot(x='alpha', y='value', hue='variable', data=sol_path)\n",
    "ax.set_title(\"Ridge Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8c1e0",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (CORE)\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `lpsa`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a81334",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c932e2c",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "Run the code below to also plot both the training and test MSE as a function of $\\alpha$. What do you notice about the MSE as we increase $\\alpha$? Which value of $\\alpha$ seems better regarding the changes on training and testing MSE values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83611fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses_path = pd.DataFrame(\n",
    "    {'alpha': alphas, 'Train': np.asarray(mses_train), 'Test': np.asarray(mses_test)}).melt(\n",
    "    id_vars = ('alpha')\n",
    ")\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x='alpha', y='value', hue='variable', data=mses_path)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "# To remove legend title\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea0348",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5affd40",
   "metadata": {},
   "source": [
    "## Tuning the penalty parameter with cross-validation\n",
    "\n",
    "We see that the value of $\\alpha$ crucially determines the performance of the ridge regression model. While `RidgeRegression()` uses the default value of `alpha=1`, this should never be used in practice. Instead, this parameter can be tuned using cross-validation. \n",
    "\n",
    "As with the polynomial models from last week, we can use `GridSearchCV` to employ k-fold cross validation to determine an optimal $\\alpha$. Remember, you can use the method `.get_params()` on your pipeline to list the parameters names to specify in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48966fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of tuning parameters\n",
    "alphas = np.linspace(0, 15, num=151)  \n",
    "\n",
    "#Pipeline\n",
    "m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge())\n",
    "\n",
    "# CV strategy\n",
    "cv = KFold(5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Grid search\n",
    "gs = GridSearchCV(m,\n",
    "    param_grid={'ridge__alpha': alphas},\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\")\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831e8e8",
   "metadata": {},
   "source": [
    "Note that we are passing `sklearn.model_selection.KFold(5, shuffle=True, random_state=1234)` to the `cv` argument rather than leaving it to its default. This is because, while not obvious, the prostate data is structured (sorted by `lpsa` value) and this way we are able to ensure that the folds are properly shuffled. Failing to do this causes *very* unreliable results from the cross validation process.\n",
    "\n",
    "Once fit, we can examine the results to determine what value of $\\alpha$ was chosen as well as examine the results of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(gs.best_estimator_, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ec750",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 8 (CORE)\n",
    "\n",
    "- How does this model compare to the performance of our baseline model? Is it better or worse?\n",
    "\n",
    "- How do the model coefficients for this model compare to the baseline model? To answer this plot the coefficients for the baseline model against the coefficients for the ridge model. Are they always higher or lower? Now, use `np.linalg.norm` to compute the $\\ell_2$ norm of the coeffcients for both models and comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538ee9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cf2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e068a9d",
   "metadata": {},
   "source": [
    "As we saw last week, it is also recommend to plot the CV scores. Although the grid search may report a best value for the parameter corresponding to the maximum CV score (e.g. min CV MSE), if the curve is relatively flat around the minimum, we may prefer the simpler model. \n",
    "\n",
    "Recall from last week that we can access the cross-validated scores (along with other results for each split) in the attribute `cv_results_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(gs.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e758f8",
   "metadata": {},
   "source": [
    "In particular, let's examining the `mean_test_score` and the `split#_test_score` keys since these are used to determine the optimal $\\alpha$.\n",
    "\n",
    "In the code below we extract these data into a data frame by selecting our columns of interest along with the $\\alpha$ values used (and transform negative MSE values into positive values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse = pd.DataFrame(\n",
    "    data = gs.cv_results_\n",
    ").filter(\n",
    "    # Extract the split#_test_score and mean_test_score columns\n",
    "    regex = '(split[0-9]+|mean)_test_score'\n",
    ").assign(\n",
    "    # Add the alphas as a column\n",
    "    alpha = alphas\n",
    ")\n",
    "\n",
    "cv_mse.update(\n",
    "    # Convert negative mses to positive\n",
    "    -1 * cv_mse.filter(regex = '_test_score')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='alpha', y='mean_test_score', data=cv_mse)\n",
    "ax.set_ylabel('CV MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a3932",
   "metadata": {},
   "source": [
    "This plot shows that the value of $\\alpha=4.9$ corresponds to the minimum of this curve. However, this plot gives us an overly confident view of this particular value of $\\alpha$. Specifically, if instead of just plotting the mean MSE across all of the validation sets, we also examine the MSE for each fold individually and the corresponding optimal value of $\\alpha$, we see that there is a lot of noise in the MSE and we should take the value $\\alpha = 4.9$ with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01d2e4",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 9 (CORE)\n",
    "\n",
    "Run the code below to plot the MSE for each validation set in the 5-fold cross validation. Why do you think that our cross validation results are unstable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2232ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cv_mse.melt(\n",
    "    id_vars=('alpha','mean_test_score'),\n",
    "    var_name='fold',\n",
    "    value_name='MSE'\n",
    ")\n",
    "\n",
    "sns.lineplot(x='alpha', y='MSE', color='black', errorbar=None, data = d)  # Plot the mean MSE in black.\n",
    "sns.lineplot(x='alpha', y='MSE', hue='fold', data = d) # Plot the curves for each fold in different colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c1f37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39483db1",
   "metadata": {},
   "source": [
    "## Tuning with RidgeCV\n",
    "\n",
    "Due to the importance of tuning the value of $\\alpha$ in ridge regression, sklearn provides a helpful function called [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) which combines `Ridge` with `GridSearchCV`. \n",
    "\n",
    "Note however, that `RidgeCV()` only allows to store all results of the cross-validation in the attribute `.cv_results_` in the case of the default leave-one-out cross validation, with option `store_cv_values=True`. So, if you want to access all results and use a cross-validation strategy other than leave-one-out, you will need to use `GridSearchCV`. \n",
    "\n",
    "After fitting using `RidgeCV()`, you can access the optimal value of $\\alpha$ through the attribute `.alpha_`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3720d",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 10 (EXTRA)\n",
    "\n",
    "Refit the model using the RidgeCV in such a way that you obtain a result similar to what was obtained by `GridSearchCV` (in terms of the optimal $\\alpha$ and test MSE).\n",
    "\n",
    "*Note: `RidgeCV` does not allow `alpha=0` for some reason, so you will need to start your grid of parameters at a small positive value.*\n",
    "\n",
    "*Note: the results will not exactly agree. This is because `GridSearchCV` will carry out standardization of features on each fold, while `RidgeCV` carries it out only once. If the standardization is relatively stable across folds, the results will agree.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3827c",
   "metadata": {},
   "source": [
    "# Lasso Regression<a id='lasso'></a>\n",
    "\n",
    "We saw that ridge regression with a wise choice of $\\alpha$ can outperform our baseline linear regression. We can now investigate if lasso can yield a more accurate or interpretable solution. Recall that lasso uses an $\\ell_1$ penalty on the coefficients, as opposed to the $\\ell_2$ penalty of ridge. \n",
    "\n",
    "The [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model is also provided by the `linear_model` submodule and similarly requires the choice of the tuning parameter `alpha` to determine the weight of the $\\ell_1$ penalty. \n",
    "\n",
    "Try running the code below with different values of $\\alpha$ to see how it effects sparsity in the coefficients and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14007019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Selected alpha value \n",
    "alpha_val = 0.15\n",
    "\n",
    "l = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Lasso(alpha = alpha_val)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(np.c_[np.append(['intercept'],l.feature_names_in_),np.round(get_coefs(l),4)])\n",
    "\n",
    "model_fit(l, X_test, y_test, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3aa6be",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "a) Plot the solution path of the coefficients as a function of $\\alpha$.\n",
    "\n",
    "b) How does this differ between the solution path for Ridge? \n",
    "\n",
    "c) Which variable seems to be the most important for predicting `lpsa`?\n",
    "\n",
    "*Note that $\\alpha = 0$ causes a warning due to the fitting method (coordinate descent) not converging well without regularization (the $\\ell_1$ penalty here). So, the grid of $\\alpha$ values needs to start at some small positive constant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b979333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a: Compute and plot the solution path\n",
    "alphas = np.linspace(0.01, 1, num=100) #We need smaller values of alpha in the grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6809e6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42e70168",
   "metadata": {},
   "source": [
    "## Tuning the Lasso penalty parameter\n",
    "\n",
    "Again, we can use the `GridSearchCV` function to tune our Lasso model and optimize the $\\alpha$ hyperparameter or use [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), which combines `Lasso` and `GridSearchCV`. \n",
    "\n",
    "Unlike `RidgeCV`, the fitted `LassoCV` has an attribute `mse_path_` providing the MSE for each fold in cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d87ce",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 12 (CORE)\n",
    "\n",
    "a) Use `LassoCV` to find the optimal value of $\\alpha$.  \n",
    "\n",
    "b) Plot the CV MSE and MSE for each fold. Comment on the stability and uncertainty of $\\alpha$ across the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a: optimal alpha\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Grid of tuning parameters\n",
    "alphas = np.linspace(0.01, 1, num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b: MSE path per fold\n",
    "# Note: CHANGE the name gs_lasso to the name of your fitted pipeline from part a\n",
    "\n",
    "fold_names =[]\n",
    "[fold_names.append('Fold '+str(i+1)) for i in range(5)]\n",
    "\n",
    "cv_mse = pd.DataFrame(\n",
    "    data = gs_lasso[\"lassocv\"].mse_path_, \n",
    "    columns = fold_names\n",
    ").assign(alpha = gs_lasso[\"lassocv\"].alphas_) \n",
    "\n",
    "d = cv_mse.melt(\n",
    "    id_vars=('alpha'),\n",
    "    var_name='CV',\n",
    "    value_name='MSE'\n",
    ")\n",
    "\n",
    "sns.lineplot(x='alpha', y='MSE', color='black', errorbar=None, data = d)  # Plot the mean MSE in black.\n",
    "sns.lineplot(x='alpha', y='MSE', hue='CV', data = d) # Plot the curves for each fold in different colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e94a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e9752fc",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 13 (CORE)\n",
    "\n",
    "Run the following code to compute the CV MSE for the linear model and compare with the CV MSE of the lasso model to suggest an optimal value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_l = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    param_grid = {},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_mean_squared_error\"\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5df4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CV MSE for baseline linear model', round(gs_l.best_score_ * -1,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951da807",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af11956d",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 14 (EXTRA)\n",
    "\n",
    "Using `ColumnTransfomer`apply standarization to all variables except the binary variable `svi`. How does the affect the lasso solution path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c547160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "alphas = np.linspace(0.01, 1, num=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa32a57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8a6ae7",
   "metadata": {},
   "source": [
    "# ElasticNet Regression<a id='elasticnet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ed46b",
   "metadata": {},
   "source": [
    "Lastly, we can use elastic net regression, which is hybrid between lasso and ridge, including both an $\\ell_1$ and $\\ell_2$ penalty. The [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) model is again provided by the `linear_model` submodule and minimizes the objective:\n",
    "$$ \\frac{1}{2N} || \\mathbf{y} - \\mathbf{X}\\mathbf{w} ||^2_2 + \\alpha \\rho ||\\mathbf{w}||_1\n",
    "+ 0.5 \\alpha (1 - \\rho) ||\\mathbf{w}||^2_2.$$\n",
    "\n",
    "In this parameterization, $\\rho$ determines relative strength of the $\\ell_1$ penalty compared to the $\\ell_2$ and is referred to as `l1_ratio` in `ElasticNet`. Thus, we can also fit ridge and lasso regression models with `ElasticNet` through appropriate choice of `l1_ratio`:\n",
    "- ridge corresponds to `l1_ratio=0`\n",
    "- lasso corresponds to `l1_ratio=1`\n",
    "\n",
    "The parameter $\\alpha$ is referred to as `alpha` in `ElasticNet` and controls the overall penalty relative the residual sum of squares. \n",
    "\n",
    "The general `ElasticNet` requires tuning of both `alpha` and `l1_ratio`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460fcd8",
   "metadata": {},
   "source": [
    "The following code plots the solution path for different choices of `l1_ratio` using the `.path()` method of `ElasticNet`. Notice how the solution paths resemble ridge and lasso for small and large values of `l1_ratio` respectively. \n",
    "\n",
    "In this case, `.path()` by default automatically selects a range of `alpha` values, except for the case when `l1_ratio = 0`, i.e. ridge regression. For ridge, you need to supply your own grid of `alpha` values through the option `path(...,alphas=myalphas)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "Xs = StandardScaler().fit_transform(X_train)\n",
    "l1r = [.1, .5, .9, 1]\n",
    "fig, ax = plt.subplots(1,4,figsize= (15,6))\n",
    "for i, l in enumerate(l1r):\n",
    "    sol_path = ElasticNet.path(Xs, y_train, l1_ratio=l)\n",
    "    d = pd.DataFrame( data = sol_path[1].T, columns = X_train.columns, index = sol_path[0])\n",
    "    d.plot(ax=ax[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc458d18",
   "metadata": {},
   "source": [
    "Again, we can use either `GridSearchCV` or [`ElasticNetCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html) to the parameters. In the following code, we use `ElasticNetCV` to tune both `alpha` and `l1_ratio`. In this case, instead of supplying a grid of `alpha` values, we let `ElasticNetCV` automatically select the values and simply specify the length of the grid through the option `ElasticNetCV(...,n_alphas=100)`. \n",
    "\n",
    "As usual, we can access the optimal parameters through the attributes `.alpha_` and `.l1_ratio`, and extract the MSE for each held out validation set through the attribute `.mse_path_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Grid of tuning parameters\n",
    "nalphas = 100 \n",
    "l1r = [0.01, .1, .5, .7, .9, .95, 1]\n",
    "\n",
    "# CV strategy\n",
    "cv = KFold(5, shuffle=True, random_state=1234)\n",
    "\n",
    "# lasso CV\n",
    "ecv = ElasticNetCV(n_alphas=nalphas, l1_ratio=l1r, cv=cv)\n",
    "\n",
    "#Pipeline\n",
    "gs_en = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        ecv)\n",
    "gs_en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c48634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"l1_ratio:\", gs_en['elasticnetcv'].l1_ratio_)\n",
    "print( \"alpha:\", gs_en['elasticnetcv'].alpha_)\n",
    "print( \"minimum value of alpha in the grid:\", np.min(gs_en['elasticnetcv'].alphas_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse_en = np.mean(gs_en[\"elasticnetcv\"].mse_path_,axis=2)\n",
    "print('CV MSE for elasticnet model', round(np.min(cv_mse_en),4))\n",
    "print('CV MSE for ridge model',round(-gs.best_score_,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08941ca6",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 15 (EXTRA)\n",
    "\n",
    "Comment on the optimal values of ElasticNet compared with our basineline, ridge, and lasso models. How does the performance of the models compare on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcd6a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7deb2244",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week05.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "MLPy Workshop 5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
